# ğŸ  Rent-Price-Prediction-using-Machine-Learning



ğŸ“˜ Overview

This project aims to predict rental prices based on property features using multiple regression models.
The goal was to evaluate various machine learning algorithms and identify the best-performing model for accurate rent prediction.

ğŸ§  Models Implemented

The following regression algorithms were implemented and compared:

Linear Regression

Lasso Regression

Ridge Regression

ElasticNet Regression

K-Nearest Neighbors (KNN) Regression

Random Forest Regression

Decision Tree Regression

âš™ï¸ Workflow

Data Preprocessing

Handled categorical variables using pd.get_dummies()

Scaled features using StandardScaler

Split dataset into training and testing sets

Model Training & Evaluation

Used K-Fold Cross Validation (k=5) for all models

Evaluated using RÂ² and Mean Squared Error (MSE)

Model Tuning

Tuned RandomForestRegressor using different n_estimators values (50â€“500)

Selected the configuration with the best average RÂ² and lowest MSE

ğŸ“Š Model Performance Comparison
Model	RÂ²	MSE
Linear Regression	0.414	765,815

Lasso Regression	0.481	693,229

Ridge Regression	0.457	717,482

ElasticNet Regression	0.485	686,536

KNN Regression	0.348	802,094

Random Forest Regression	0.606	541,277

Decision Tree Regression	0.454	696,680


ğŸ”§ Model Tuning Results (Random Forest)

n_estimators	RÂ² Mean	RÂ² Std	MSE Mean

50	0.606	0.175	541,277

100	0.612	0.175	534,691

200	0.615	0.176	531,715

300	0.615	0.176	531,436

400	0.615	0.177	531,565

500	0.616	0.177	530,465


âš¡ Key Insights

Random Forest outperformed all other models with an RÂ² of 0.616 and the lowest MSE.

Feature scaling improved model performance for Lasso, Ridge, and ElasticNet.

Using n_jobs = -1 parallelized model training for faster computation.

ğŸ§© Technologies Used

Python

Pandas, NumPy â€” Data manipulation

Scikit-learn â€” Model building & evaluation

Matplotlib, Seaborn â€” Visualization

Jupyter Notebook â€” Experimentation


ğŸ§¾ Results & Conclusion

The Random Forest Regressor achieved the best overall performance.

Future improvements can include feature engineering, hyperparameter optimization, and advanced ensemble techniques to push RÂ² beyond 0.65+.

ğŸ‘¨â€ğŸ’» Author

Jay Panchal
ğŸ“§ Email - panchaljay2711@gmail.com

LinkedIn - https://www.linkedin.com/in/jay-panchal-396443176

ğŸ’¼ Machine Learning
